<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: devops | pfigue]]></title>
  <link href="http://pfigue.github.com/blog/categories/devops/atom.xml" rel="self"/>
  <link href="http://pfigue.github.com/"/>
  <updated>2015-06-18T15:24:31+02:00</updated>
  <id>http://pfigue.github.com/</id>
  <author>
    <name><![CDATA[pfigue]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[File Descriptors and ElasticSearch]]></title>
    <link href="http://pfigue.github.com/blog/2015/03/06/file-descriptors-and-elasticsearch/"/>
    <updated>2015-03-06T16:07:00+01:00</updated>
    <id>http://pfigue.github.com/blog/2015/03/06/file-descriptors-and-elasticsearch</id>
    <content type="html"><![CDATA[<h1>ElasticSearch consumes an high number of file descriptors.</h1>

<p>It is quite easy to break an ES cluster by having a low number of available
file descriptors.</p>

<p>When accessing indices ES will use FDs, but when connecting
with other members of the cluster or with clients, it will need also to create
sockets, and therefore, to use FDs.</p>

<p>In the web I read often 65000 as the minimal number of available FD that should
be provided. In my systems I use to configure at least one million.</p>

<h1>How to check information about file descriptors?</h1>

<h2>Via REST API</h2>

<p>We can ask each node about information on the cluster nodes:</p>

<pre><code>$ curl -s "http://127.0.0.1:9200/_nodes/process?pretty"
[...]
"v5_G7FNvRBOLpS1MMByX4w": {
    "process": {
    "mlockall": true,
**  "max_file_descriptors": 200000,**
    "id": 2165,
    "refresh_interval_in_millis": 1000
},
"name": "elastic2",
[...]
</code></pre>

<p>Or if we know the identifier of a node we can also filter:</p>

<pre><code>$ curl -s "http://127.0.0.1:9200/_nodes/process?pretty" | jq '.nodes.rsYeYNh2STqF5KyzFiUFqQ.process.max_file_descriptors'
200000
</code></pre>

<p>Maybe <a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cat.html">one of the <em>cat</em> APIs</a> has some way of reporting current number of used FDs, instead of the max, which is what I can see in the previous call.</p>

<h2>With ElasticSearch Plugins</h2>

<p>ElasticSearch has some so-named <a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-plugins.html#site">site plugins</a>:</p>

<ul>
<li>With <a href="https://github.com/royrusso/elasticsearch-HQ">HQ</a> I can see the number of currently <strong>opened file descriptors</strong>.</li>
<li>With <a href="https://github.com/lukas-vlcek/bigdesk">BigDesk</a> I have a graph of the <strong>number of FD</strong> and the <strong>max allowed FDs</strong>.</li>
<li>With <a href="https://github.com/mobz/elasticsearch-head">Head</a> we just have the same information we can have asking the API from the command line.</li>
</ul>


<h2>With the ProcFS</h2>

<p>Or in each server we can get the PID of each ES instance and ask the procfs
about the <strong>effective limits</strong>:</p>

<pre><code>$ ps aux | grep java
elastic+ 17586 17.4 54.9 8102680 4225596 ?     SLl  Mar04 522:30 /usr/bin/java -Xms3584M -Xmx3584M [...]
$
$ cat /proc/17586/limits
Limit                     Soft Limit           Hard Limit           Units     
Max cpu time              unlimited            unlimited            seconds   
Max file size             unlimited            unlimited            bytes     
Max data size             unlimited            unlimited            bytes     
Max stack size            8388608              unlimited            bytes     
Max core file size        0                    unlimited            bytes     
Max resident set          unlimited            unlimited            bytes     
Max processes             59935                59935                processes 
**Max open files            200000               200000               files**
Max locked memory         unlimited            unlimited            bytes     
Max address space         unlimited            unlimited            bytes     
Max file locks            unlimited            unlimited            locks     
Max pending signals       59935                59935                signals   
Max msgqueue size         819200               819200               bytes     
Max nice priority         0                    0                    
Max realtime priority     0                    0                    
Max realtime timeout      unlimited            unlimited            us
$
</code></pre>

<p>That figure is the important one, when configuring limits for an ES server.</p>

<h2>System calls: getrlimit and prlimit</h2>

<p>The <code>getrlimit</code> and <code>prlimit</code> system calls can be also used to fetch the current
and maximal figures of file descriptors.</p>

<h1>How to configure the number of total available FDs</h1>

<h2>System-wide configuration</h2>

<p>Via SysFS we can tune those kernel parameters[9] that affect the entire system:</p>

<pre><code>$ sysctl fs.file-nr
fs.file-nr = 9984       0       1149332
</code></pre>

<p>Where 9984 is the current number of opened file descriptors.
<code>man sysctl</code> can throw some help on how to change the max number.</p>

<h2>Shell session configuration</h2>

<p>The shell imposes another additional limitation to the users and its processes
via ulimit [10]:</p>

<pre><code>$ ulimit -n
1000000
</code></pre>

<p>Any process I launch in this shell session inherits that limit and can open so
many FD as it wants solong neither this limit nor the kernel limit are surpassed.</p>

<h2>PAM authentication</h2>

<p>The file <code>/etc/security/limits.conf</code> can contain some rules like:</p>

<pre><code>* soft nofile 1024000
* hard nofile 1024000
</code></pre>

<p>to define soft and hard limits for the number of files.</p>

<p>Changes in this file require the users to log out and log in again to take
effect [10].</p>

<p>Nonetheless, not every process is controlled by these PAM rules. Afaik, only
processes which were born in a login shell[4] will inherit these limits.</p>

<p>Processes started from Upstart do not inherit these limits. See [3].</p>

<h2>Upstart tasks</h2>

<p>The <em>stanza</em> <code>limit nofile 4096 4096</code>[1] in the Upstart task in /etc/init/ sets
the limit for the processes started in the Upstart job.</p>

<p>Note that the PAM limits (<code>/etc/security/limits.conf</code>) are not applied to these
jobs, so this <code>limit</code> statement is the only way to be sure these processes will
have enough FD available.</p>

<p>See [2] and [3].</p>

<h2>ElasticSearch configuration</h2>

<p>I can see <code>MAX_OPEN_FILES=200000</code> in <code>/etc/default/elasticsearch</code>. That file is
read whenever <code>/etc/init.d/elasticsearch</code> is run, and that parameter is applied
to <code>ulimit -n</code>, so be sure you write there a meaningful number.</p>

<h2>Tweaking a running process</h2>

<p>The same as we could read the limits for a process with <code>prlimit</code> or <code>getrlimit</code>
system calls, we can also set those limits with <code>prlimit</code> or <code>setrlimit</code>.</p>

<p>If we don&rsquo;t want to implement a program to use those system calls we can:</p>

<pre><code>1. Use the `prlimit` command of the util-linux package.
2. Use a short [solution Lars Windolf wrote](http://lzone.de/Debian%20Ubuntu%20ulimit%20Check%20List)[5,6]
</code></pre>

<p>To use the <code>prlimit</code> command[7] you will need <code>util-linux</code> package[8] installed, version >=2.21.
Ubuntu 14.04 is currently lacking this program as it uses version 2.20.</p>

<p>The great advantage of this system calls is that we don&rsquo;t need to restart the
process to change the limits.</p>

<h1>Refs/Notes</h1>

<ol>
<li><a href="http://upstart.ubuntu.com/wiki/Stanzas#limit">Official Upstart Stanzas Doc &ndash; limit</a></li>
<li><a href="http://bryanmarty.com/2012/02/10/setting-nofile-limit-upstart/">Setting the nofile limit in upstart</a></li>
<li><a href="https://coderwall.com/p/myodcq/setting-max-file-descriptors-and-other-limits-with-upstart-on-debian-ubuntu">Setting max file descriptors and other limits with upstart on debian/ubuntu</a></li>
<li>and maybe those from SysV init.d[3], although my ES service runs from <code>/etc/init.d/elasticsearch</code> and follows the same limits as Upstart tasks (Ubuntu 14.04)</li>
<li><a href="http://lzone.de/Debian%20Ubuntu%20ulimit%20Check%20List">The Debian/Ubuntu ulimit Check List</a></li>
<li><a href="https://gist.github.com/pfigue/976d781c282d7780412d">Gist &ndash; set_limit_nofile.c</a></li>
<li><a href="http://manpages.courier-mta.org/htmlman1/prlimit.1.html">Manpage &ndash; prlimit</a></li>
<li><a href="https://github.com/karelzak/util-linux">Github: util-linux is a random collection of Linux utilities</a></li>
<li><a href="http://www.cyberciti.biz/tips/linux-procfs-file-descriptors.html">Linux: Find Out How Many File Descriptors Are Being Used</a></li>
<li><a href="http://www.unixmantra.com/2013/06/resource-limits-on-unix-systems-ulimit.html">Resource limits on UNIX systems (ulimit)</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Vagrant with LXC Linux Containers]]></title>
    <link href="http://pfigue.github.com/blog/2014/01/25/using-vagrant-with-lxc-linux-containers/"/>
    <updated>2014-01-25T15:44:00+01:00</updated>
    <id>http://pfigue.github.com/blog/2014/01/25/using-vagrant-with-lxc-linux-containers</id>
    <content type="html"><![CDATA[<h2>Containers</h2>

<p>LXC Containers are a relatively new way of virtualization for Linux environments. Solaris had containers since several years ago, but in Linux they appeared around 5 years ago, afaik.</p>

<p>The point of virtualizing using containers instead of virtual machines is that, both, guest and host, use the same instance of the kernel, which translates in <strong>far less time to boot and far less resources used</strong>.</p>

<p>A container is just a set of resources and processes running in a different namespace, eventually with some limitations (CPU or I/O usage, memory used, etc.).</p>

<p>Afair, there are/were 2 implementations of containers for linux: OpenVZ and LXC. Without being an expert in the area, I&rsquo;d say that LXC has much better maintenance nowadays than OpenVZ.</p>

<h2>LXC Containers in Ubuntu</h2>

<p>In Ubuntu 12.04 we can install LXC as usual with <code>sudo apt-get install lxc</code>. As root, we can list the containers we have in every state with <code>sudo lxc-list</code>.</p>

<p>Usually, <a href="https://help.ubuntu.com/lts/serverguide/lxc.html#lxcbr0">once <strong>lxc</strong> is installed, we should find a new network bridge</a>: <em>lxcbr0</em>. You can check if it is present with:</p>

<pre><code>$ ifconfig lxcbr0 2&gt; /dev/null 1&gt;&amp;2  &amp;&amp; echo 'lxc interface is present' || echo 'no lxc interface'
lxc interface is present
$
</code></pre>

<p>Before start to use containers with Vagrant, we should <a href="https://github.com/fgrehm/vagrant-lxc/wiki/Avoiding-%27sudo%27-passwords">work around a bug in sudo (Ubuntu 12.04)</a>.</p>

<p>With <code>sudo visudo</code> you can add the line:</p>

<pre><code>Defaults        !tty_tickets
</code></pre>

<p>to the sudoers file. It will solve the bug.</p>

<h2>Vagrant with the LXC provider</h2>

<p>Let&rsquo;s create a new vagrant project:</p>

<pre><code>$ mkdir vagrant-lxc
$ cd vagrant-lxc/
$ vagrant init test-lxc http://dl.dropbox.com/u/13510779/lxc-precise-amd64-2013-07-12.box
</code></pre>

<p>Where the URL is an <a href="http://dl.dropbox.com/u/13510779/lxc-precise-amd64-2013-07-12.box">Ubuntu 12.04 amd64 box <strong>for LXC</strong></a> from <a href="http://vagrantbox.es/">vagrantbox.es</a>.</p>

<p>You can use the same <a href="http://pfigue.github.io/blog/2014/01/25/my-first-steps-with-vagrant/">Vagrantfile I used in the previous article</a>, but notice that the <code>config.vm.network</code> parameter is ignored by this provider.
At the moment you can only forward port, but not bridge any interface.</p>

<p>Now you can <a href="https://github.com/fgrehm/vagrant-lxc/blob/master/README.md">install there the <strong>vagrant-lxc</strong> plugin</a> with <code>vagrant plugin install vagrant-lxc</code>.</p>

<p>Once done you should already be able to fire up that box with LXC:</p>

<pre><code>$ vagrant up --provider=lxc
[default] Warning! The LXC provider doesn't support any of the Vagrant public / private
network configurations (ex: `config.vm.network :private_network, ip: "some-ip"`).
They will be silently ignored.
</code></pre>

<p><code>vagrant ssh</code> from the same <code>vagrant-lxc/</code> directory to check it, or <code>sudo lxc-list</code> or <code>lxc-ps --lxc</code> to visualice the running containers.</p>

<h2>Advantages</h2>

<p>The time needed to boot up a Vagrant box with LXC is really much lower than using VirtualBox. This translates in faster try-error loop, faster feedback and <a href="http://pfigue.github.io/blog/2014/01/25/my-first-steps-with-vagrant/">faster development of Puppet Manifest, server configuration</a>, <a href="http://pfigue.github.io/blog/2014/01/25/my-first-steps-with-vagrant/">faster testing of packages</a>, etc.</p>

<p>References:</p>

<ul>
<li> <a href="https://help.ubuntu.com/lts/serverguide/lxc.html#lxcbr0">LXC &ndash; lxcbr0</a></li>
<li> <a href="https://github.com/fgrehm/vagrant-lxc/blob/master/README.md">README for vagrant-lxc plugin</a></li>
<li> <a href="https://github.com/fgrehm/vagrant-lxc/wiki/Avoiding-%27sudo%27-passwords">vagrant-lxc &ndash; BUG &ndash; Avoiding &lsquo;sudo&rsquo; passwords</a></li>
<li> <a href="http://pfigue.github.io/blog/2014/01/25/my-first-steps-with-vagrant/">My first steps with Vagrant</a></li>
<li> <a href="http://fabiorehm.com/blog/2013/04/28/lxc-provider-for-vagrant/">LXC provider for Vagrant</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[My first steps with Vagrant]]></title>
    <link href="http://pfigue.github.com/blog/2014/01/25/my-first-steps-with-vagrant/"/>
    <updated>2014-01-25T12:52:00+01:00</updated>
    <id>http://pfigue.github.com/blog/2014/01/25/my-first-steps-with-vagrant</id>
    <content type="html"><![CDATA[<h2>Why Vagrant?</h2>

<p>Lately I was developing my-own-solution &trade; for package deployment and now I&rsquo;d to check if I can install system packages without problems.</p>

<p><strong>Vagrant is a tool that lets me create, destroy and recreate Virtual Machines very easily</strong>, so I can in less than 5 minutes, create a new VM with a fresh install of Ubuntu 12.04 and install there the package I want to test.</p>

<p>It can also be useful to test system configuration (e.g. Puppet manifests or Chef recipes, nginx config., etc.) without playing with the production environment.</p>

<p>Another interesting use (and the reason I want to add Vagrant to the development toolchain my team uses at work) is to provide every developer with the option of packing his/her branch and deploy it with the help of Vagrant into a fresh VM, locally on his/her laptop.</p>

<p>Once deployed there, unit and functional test could be run in/against that VM, to be sure that every single branch sent to staging passed a minimal set of requirements.</p>

<h2>Replacing the old-Ubuntu-12.04 Vagrant install with a factory Vagrant.</h2>

<p>Long time ago I installed <strong>vagrant</strong> via <code>sudo apt-get install vagrant</code>, and since then I was getting an error whenever I tried to start a new project:</p>

<pre><code>$ vagrant init test1 http://cloud-images.ubuntu.com/vagrant/precise/current/precise-server-cloudimg-amd64-vagrant-disk1.box
/home/pablo/.rvm/rubies/ruby-2.0.0-p353/lib/ruby/site_ruby/2.0.0/rubygems/core_ext/kernel_require.rb:55:in `require': cannot load such file -- log4r (LoadError)
        from /home/pablo/.rvm/rubies/ruby-2.0.0-p353/lib/ruby/site_ruby/2.0.0/rubygems/core_ext/kernel_require.rb:55:in `require'
        from /usr/bin/vagrant:2:in `&lt;main&gt;'
</code></pre>

<p>I decided to remove the version of Vagrant shipped with Ubuntu 12.04, and install a new one from <a href="http://www.vagrantup.com/downloads.html">the debian package I could get via HashiCorp</a>:</p>

<pre><code>$ sudo apt-get remove --purge vagrant
$ wget "https://dl.bintray.com/mitchellh/vagrant/vagrant_1.4.3_x86_64.deb"
$ sudo dpkg -i vagrant_1.4.3_x86_64.deb
</code></pre>

<p>Due to the first installation of Vagrant, probably all the dependencies were already satisfied and I didn&rsquo;t have to install anything else.</p>

<h2>First project</h2>

<p>In Vagrant, you create a new directory for every project. In that directory will live a <code>Vagrantfile</code> with all the configuration of the VM(s) for that project.</p>

<pre><code>$ mkdir vagrant
$ cd vagrant/
$ vagrant init test1 http://cloud-images.ubuntu.com/vagrant/precise/current/precise-server-cloudimg-amd64-vagrant-disk1.box
A `Vagrantfile` has been placed in this directory. You are now
ready to `vagrant up` your first virtual environment! Please read
the comments in the Vagrantfile as well as documentation on
`vagrantup.com` for more information on using Vagrant.
</code></pre>

<p>If you are going to use Vagrant as a testing/deployment solution at work, you may find interesting to track the Vagrantfile via a <em>git</em> repository.</p>

<p>Let&rsquo;s edit the Vagrantfile a little bit to redirect the port 8080 on my laptop to the port 8000 in the test1 VM. The result would look like:</p>

<pre><code># Vagrantfile API/syntax version. Don't touch unless you know what you're doing!
VAGRANTFILE_API_VERSION = "2"

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
  config.vm.box = "test1"
  config.vm.box_url = "http://cloud-images.ubuntu.com/vagrant/precise/current/precise-server-cloudimg-amd64-vagrant-disk1.box"
  config.vm.network :forwarded_port, guest: 8000, host: 8080
end
</code></pre>

<p>The URL <a href="http://cloud-images.ubuntu.com/vagrant/precise/current/precise-server-cloudimg-amd64-vagrant-disk1.box">http://cloud-images.ubuntu.com/vagrant/precise/current/precise-server-cloudimg-amd64-vagrant-disk1.box</a> is a boxed <a href="http://cloud-images.ubuntu.com/vagrant/precise/current/precise-server-cloudimg-amd64-vagrant-disk1.box">Ubuntu 12.04 amd64 (for VirtualBox)</a> that I found in <a href="http://www.vagantbox.es">vagrantbox.es</a>.</p>

<p>Once the file is edited and saved, if now I run <code>vagrant up</code>, as it is the first time I fire that VM up, it will fetch the .box file (around 400M big) and create the VM.</p>

<p>Per default, the virtualization <em>provider</em> is VirtualBox, but I&rsquo;ve seen boxes for VMWare and for Linux Containers (LXC).</p>

<h2>Testing the port redirect</h2>

<p>Typing <code>vagrant ssh</code> in the shell inside the <code>vagrant/</code> directory will establish an SSH session with the VM.</p>

<p>Once there you can type this to get netcat listening on port 8000 of the VM:</p>

<pre><code>$ nc -l 0.0.0.0 8000
</code></pre>

<p>In other terminal you can try to access the port 8080 of your computer (the host of the VM), like:</p>

<pre><code>$ curl http://127.0.0.1:8080
</code></pre>

<p>It should redirect the packets from 8080/tcp in the host to 8000/tcp in the VM, and you should be able to see the HTTP request in the terminal where netcat is running.</p>

<h2>Trying a network bridge instead of a port redirect</h2>

<p>If the VM will need to access a computer in the same network as the host (e.g. a database server), or the VM will be accessed by a computer different than the host (e.g. a <a href="http://code.google.com/p/selenium/wiki/Grid2">Selenium Grid</a>), you may find easier to install a network bridge better than redirect some ports.</p>

<p>With that purpose in mind, the Vagrantfile should be something like:</p>

<pre><code># Vagrantfile API/syntax version. Don't touch unless you know what you're doing!
VAGRANTFILE_API_VERSION = "2"

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
  config.vm.box = "test1"
  config.vm.box_url = "http://cloud-images.ubuntu.com/vagrant/precise/current/precise-server-cloudimg-amd64-vagrant-disk1.box"
  config.vm.network :public_network
end
</code></pre>

<p>The new Vagrant configuration can be applied with <code>vagrant reload</code>. That command will stop (in a polite and slow way) and start again the VMs, asking this time the user in the command-line for which network interface will be the VM bridged to.</p>

<p>Once the VM is completelly booted, from a <code>vagrant ssh</code> session, the available interfaces can be checked:</p>

<pre><code>$ ifconfig -a | perl -ne 'print "$1\n" if m/^(\S+)/'
eth0
eth1
lo
$ ifconfig eth1 | grep inet\ addr
          inet addr:192.168.0.105  Bcast:192.168.0.255  Mask:255.255.255.0
</code></pre>

<p>So, my VM got the IP address 192.168.0.105. If I start a netcat in port 90/tcp with <code>sudo nc -l 0.0.0.0 90</code></p>

<p>Then, I should be able to connect to port 90/tcp of the VM from my laptop, with something like <code>telnet 192.168.0.105 90</code>.</p>

<p>Once you stop playing with Vagrant, you can stop the VM via <code>vagrant halt</code>.</p>

<p>References:</p>

<ul>
<li><p> <a href="http://www.jedi.be/blog/2011/03/28/using-vagrant-as-a-team/">Using Vagrant as a Team &ndash; Just Enough Developed Infrastructure</a></p></li>
<li><p> <a href="http://www.vagrantbox.es/">A list of base boxes for Vagrant</a></p></li>
<li><p> <a href="http://www.vagrantup.com/downloads.html">Vagrant Packages and Tarballs</a></p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A custom Facter fact to read AWS User-Data]]></title>
    <link href="http://pfigue.github.com/blog/2013/11/28/a-custom-facter-fact-to-read-aws-user-data/"/>
    <updated>2013-11-28T12:35:00+01:00</updated>
    <id>http://pfigue.github.com/blog/2013/11/28/a-custom-facter-fact-to-read-aws-user-data</id>
    <content type="html"><![CDATA[<h2>I need to do the provisioning of some AWS EC2 Instances depending on their role.</h2>

<p>Some of them will need to install some packages with some configuration, others will need different packages and different configuration, depending on the role they will adopt in the new system.</p>

<p>For this reason I need Puppet to behave in a different way depending on a <strong>fact</strong>, the <em>server flavour</em> of the server who is being provisioned.</p>

<h2>How to communicate the role to Puppet</h2>

<p>The first exercise would be to learn how <a href="http://pfigue.github.io/blog/2013/11/27/how-to-add-custom-facts-to-facter/">to write a new dummy fact for Facter</a>, so we are sure we can write facts for Puppet.</p>

<p>The second goal would be to write a new fact that actually reads the role of the server.</p>

<h3>Appending custom information to an EC2 instance.</h3>

<p>I communicate the role (and other information) to a new server <strong>via User-Data</strong>, whenever I <em>Launch a New Instance</em> from the AWS Web Console.
During the 3rd step, named <em>Configure Instance Details</em>, I have the option to define <em>Advanced Details</em>, one of them the <em>User data</em>.</p>

<p>Once the instance is running, I <strong>can&rsquo;t change that User-Data</strong>. If I want to, I need to destroy that instance and create a new one with the new data.</p>

<p>An <strong>alternative to User-Data is to use <em>Tags</em></strong>, and then the EC2 Command-Line Tools to access and read information from those Tags. But you should mind that there is a <strong>limit of 10 tags per server</strong>, so it may not be the best solution. On the other hand, <strong>you can change the tags while the Instance is running</strong>. No need to destroy it.</p>

<p>I upload a YAML file with all the information, and then, once the new EC2 Instance is up, from the instance itself I can get that YAML file:</p>

<pre><code>$ /usr/bin/curl --silent "http://169.254.169.254/latest/user-data/"
instance:
    flavor: application-server
$
</code></pre>

<p>With <a href="https://gist.github.com/pfigue/7690424">some python scripting like</a>:</p>

<pre><code>import sys
import yaml

if __name__ == '__main__':
    contents = sys.stdin.read()
    document = yaml.load(contents)
    for key in sys.argv[1:]:
            try:
                    document = document[key]
            except KeyError:
                    sys.exit(1)
    sys.stdout.write(str(document))
    sys.stdout.flush()
    sys.exit(0)
</code></pre>

<p>I can provide the script the YAML via stdin and then ask for some data:</p>

<pre><code>$ curl --silent "http://169.254.169.254/latest/user-data/" | python ./ec2-user-data-parser.py instance flavor
application-server
$
</code></pre>

<h3>Writting the Facter <em>fact</em></h3>

<p>Writting a new fact is now straight forward (I saved the python script in <code>/root/tools/ec2-user-data-parser.py</code>):</p>

<pre><code># server_flavor.rb

Facter.add("server_flavor") do
  setcode do
    Facter::Util::Resolution.exec('/usr/bin/curl --silent "http://169.254.169.254/latest/user-data/" | /usr/bin/python /root/tools/ec2-user-data-parser.py instance flavor')
  end
end
</code></pre>

<p>I save the code to <code>/root/facter/server_flavor.rb</code> and then test it:</p>

<pre><code>$ FACTERLIB=/root/facter/ facter server_flavor
application-server
$
</code></pre>

<p>I may need still to add some <code>export FACTERLIB=/root/facter/</code> to <code>/root/.bashrc</code>, but now my puppet manifests will know which kind of Amazon servers they are provisioning.</p>

<h2>References:</h2>

<ul>
<li><a href="https://gist.github.com/pfigue/7690424">The code in a Gist</a></li>
<li><a href="http://docs.puppetlabs.com/guides/custom_facts.html">Puppet Docs: Custom Facts</a></li>
<li><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AESDG-chapter-instancedata.html">AWS Instance Meta- and User- Data</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to add custom facts to Facter]]></title>
    <link href="http://pfigue.github.com/blog/2013/11/27/how-to-add-custom-facts-to-facter/"/>
    <updated>2013-11-27T18:55:00+01:00</updated>
    <id>http://pfigue.github.com/blog/2013/11/27/how-to-add-custom-facts-to-facter</id>
    <content type="html"><![CDATA[<h2>What is Facter?</h2>

<p>It is a Ruby tool that informs you about facts.</p>

<p>As far as I know, it is used often from Puppet, to know about properties of the environment you are going to provision.</p>

<p>Those properties can be things like <em>Operating System</em>, <em>Version of the kernel</em>, <em>Architecture</em>, <em>Amazon EC2 Instance ID</em>, <em>Available memory</em>, <em>Number of processors</em>, etc.</p>

<p>The <code>facter</code> tool by default has some values like those above: you can run <code>facter</code> in the shell to get a list of <em>facts</em>.</p>

<p>From a Puppet Manifest you could write statements dependent on those facts.
For example, to install some package when the manifest is applied to a Debian system, and install a different package when applied to a Red Hat one.</p>

<h2>Do you need to define your own Facts?</h2>

<p><strong>Facter</strong> looks for the <em>FACTERLIB</em> environment variable, which may point to a directory, if it is defined.</p>

<p>If you put in that directory a file named <code>hardware_platform.rb</code> (note the <em>.rb</em> extension), and inside you define:</p>

<pre><code># hardware_platform.rb

Facter.add("hardware_platform") do
    setcode do
        Facter::Util::Resolution.exec('/bin/uname -i')
    end
end
</code></pre>

<p>There will appear a new <em>Fact</em> called <em>hardware_platform</em>:</p>

<pre><code># facter | grep -i platform
hardware_platform =&gt; x86_64
#
</code></pre>

<p>You can invoke it also via <code>facter hardware_platform</code>:</p>

<pre><code># facter hardware_platform
x86_64
#
</code></pre>

<p>Pay attention, that the file name and the Fact name are the same. An the file extension is <em>.rb</em>.</p>

<p>In this way, you can have your own facts for your Puppet manifests.</p>

<p>Puppet Docs has much <a href="http://docs.puppetlabs.com/guides/custom_facts.html">more information</a>.</p>
]]></content>
  </entry>
  
</feed>
