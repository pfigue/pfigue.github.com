<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[pfigue]]></title>
  <link href="http://pfigue.github.com/atom.xml" rel="self"/>
  <link href="http://pfigue.github.com/"/>
  <updated>2015-06-18T15:24:31+02:00</updated>
  <id>http://pfigue.github.com/</id>
  <author>
    <name><![CDATA[pfigue]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[File Descriptors and ElasticSearch]]></title>
    <link href="http://pfigue.github.com/blog/2015/03/06/file-descriptors-and-elasticsearch/"/>
    <updated>2015-03-06T16:07:00+01:00</updated>
    <id>http://pfigue.github.com/blog/2015/03/06/file-descriptors-and-elasticsearch</id>
    <content type="html"><![CDATA[<h1>ElasticSearch consumes an high number of file descriptors.</h1>

<p>It is quite easy to break an ES cluster by having a low number of available
file descriptors.</p>

<p>When accessing indices ES will use FDs, but when connecting
with other members of the cluster or with clients, it will need also to create
sockets, and therefore, to use FDs.</p>

<p>In the web I read often 65000 as the minimal number of available FD that should
be provided. In my systems I use to configure at least one million.</p>

<h1>How to check information about file descriptors?</h1>

<h2>Via REST API</h2>

<p>We can ask each node about information on the cluster nodes:</p>

<pre><code>$ curl -s "http://127.0.0.1:9200/_nodes/process?pretty"
[...]
"v5_G7FNvRBOLpS1MMByX4w": {
    "process": {
    "mlockall": true,
**  "max_file_descriptors": 200000,**
    "id": 2165,
    "refresh_interval_in_millis": 1000
},
"name": "elastic2",
[...]
</code></pre>

<p>Or if we know the identifier of a node we can also filter:</p>

<pre><code>$ curl -s "http://127.0.0.1:9200/_nodes/process?pretty" | jq '.nodes.rsYeYNh2STqF5KyzFiUFqQ.process.max_file_descriptors'
200000
</code></pre>

<p>Maybe <a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cat.html">one of the <em>cat</em> APIs</a> has some way of reporting current number of used FDs, instead of the max, which is what I can see in the previous call.</p>

<h2>With ElasticSearch Plugins</h2>

<p>ElasticSearch has some so-named <a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-plugins.html#site">site plugins</a>:</p>

<ul>
<li>With <a href="https://github.com/royrusso/elasticsearch-HQ">HQ</a> I can see the number of currently <strong>opened file descriptors</strong>.</li>
<li>With <a href="https://github.com/lukas-vlcek/bigdesk">BigDesk</a> I have a graph of the <strong>number of FD</strong> and the <strong>max allowed FDs</strong>.</li>
<li>With <a href="https://github.com/mobz/elasticsearch-head">Head</a> we just have the same information we can have asking the API from the command line.</li>
</ul>


<h2>With the ProcFS</h2>

<p>Or in each server we can get the PID of each ES instance and ask the procfs
about the <strong>effective limits</strong>:</p>

<pre><code>$ ps aux | grep java
elastic+ 17586 17.4 54.9 8102680 4225596 ?     SLl  Mar04 522:30 /usr/bin/java -Xms3584M -Xmx3584M [...]
$
$ cat /proc/17586/limits
Limit                     Soft Limit           Hard Limit           Units     
Max cpu time              unlimited            unlimited            seconds   
Max file size             unlimited            unlimited            bytes     
Max data size             unlimited            unlimited            bytes     
Max stack size            8388608              unlimited            bytes     
Max core file size        0                    unlimited            bytes     
Max resident set          unlimited            unlimited            bytes     
Max processes             59935                59935                processes 
**Max open files            200000               200000               files**
Max locked memory         unlimited            unlimited            bytes     
Max address space         unlimited            unlimited            bytes     
Max file locks            unlimited            unlimited            locks     
Max pending signals       59935                59935                signals   
Max msgqueue size         819200               819200               bytes     
Max nice priority         0                    0                    
Max realtime priority     0                    0                    
Max realtime timeout      unlimited            unlimited            us
$
</code></pre>

<p>That figure is the important one, when configuring limits for an ES server.</p>

<h2>System calls: getrlimit and prlimit</h2>

<p>The <code>getrlimit</code> and <code>prlimit</code> system calls can be also used to fetch the current
and maximal figures of file descriptors.</p>

<h1>How to configure the number of total available FDs</h1>

<h2>System-wide configuration</h2>

<p>Via SysFS we can tune those kernel parameters[9] that affect the entire system:</p>

<pre><code>$ sysctl fs.file-nr
fs.file-nr = 9984       0       1149332
</code></pre>

<p>Where 9984 is the current number of opened file descriptors.
<code>man sysctl</code> can throw some help on how to change the max number.</p>

<h2>Shell session configuration</h2>

<p>The shell imposes another additional limitation to the users and its processes
via ulimit [10]:</p>

<pre><code>$ ulimit -n
1000000
</code></pre>

<p>Any process I launch in this shell session inherits that limit and can open so
many FD as it wants solong neither this limit nor the kernel limit are surpassed.</p>

<h2>PAM authentication</h2>

<p>The file <code>/etc/security/limits.conf</code> can contain some rules like:</p>

<pre><code>* soft nofile 1024000
* hard nofile 1024000
</code></pre>

<p>to define soft and hard limits for the number of files.</p>

<p>Changes in this file require the users to log out and log in again to take
effect [10].</p>

<p>Nonetheless, not every process is controlled by these PAM rules. Afaik, only
processes which were born in a login shell[4] will inherit these limits.</p>

<p>Processes started from Upstart do not inherit these limits. See [3].</p>

<h2>Upstart tasks</h2>

<p>The <em>stanza</em> <code>limit nofile 4096 4096</code>[1] in the Upstart task in /etc/init/ sets
the limit for the processes started in the Upstart job.</p>

<p>Note that the PAM limits (<code>/etc/security/limits.conf</code>) are not applied to these
jobs, so this <code>limit</code> statement is the only way to be sure these processes will
have enough FD available.</p>

<p>See [2] and [3].</p>

<h2>ElasticSearch configuration</h2>

<p>I can see <code>MAX_OPEN_FILES=200000</code> in <code>/etc/default/elasticsearch</code>. That file is
read whenever <code>/etc/init.d/elasticsearch</code> is run, and that parameter is applied
to <code>ulimit -n</code>, so be sure you write there a meaningful number.</p>

<h2>Tweaking a running process</h2>

<p>The same as we could read the limits for a process with <code>prlimit</code> or <code>getrlimit</code>
system calls, we can also set those limits with <code>prlimit</code> or <code>setrlimit</code>.</p>

<p>If we don&rsquo;t want to implement a program to use those system calls we can:</p>

<pre><code>1. Use the `prlimit` command of the util-linux package.
2. Use a short [solution Lars Windolf wrote](http://lzone.de/Debian%20Ubuntu%20ulimit%20Check%20List)[5,6]
</code></pre>

<p>To use the <code>prlimit</code> command[7] you will need <code>util-linux</code> package[8] installed, version >=2.21.
Ubuntu 14.04 is currently lacking this program as it uses version 2.20.</p>

<p>The great advantage of this system calls is that we don&rsquo;t need to restart the
process to change the limits.</p>

<h1>Refs/Notes</h1>

<ol>
<li><a href="http://upstart.ubuntu.com/wiki/Stanzas#limit">Official Upstart Stanzas Doc &ndash; limit</a></li>
<li><a href="http://bryanmarty.com/2012/02/10/setting-nofile-limit-upstart/">Setting the nofile limit in upstart</a></li>
<li><a href="https://coderwall.com/p/myodcq/setting-max-file-descriptors-and-other-limits-with-upstart-on-debian-ubuntu">Setting max file descriptors and other limits with upstart on debian/ubuntu</a></li>
<li>and maybe those from SysV init.d[3], although my ES service runs from <code>/etc/init.d/elasticsearch</code> and follows the same limits as Upstart tasks (Ubuntu 14.04)</li>
<li><a href="http://lzone.de/Debian%20Ubuntu%20ulimit%20Check%20List">The Debian/Ubuntu ulimit Check List</a></li>
<li><a href="https://gist.github.com/pfigue/976d781c282d7780412d">Gist &ndash; set_limit_nofile.c</a></li>
<li><a href="http://manpages.courier-mta.org/htmlman1/prlimit.1.html">Manpage &ndash; prlimit</a></li>
<li><a href="https://github.com/karelzak/util-linux">Github: util-linux is a random collection of Linux utilities</a></li>
<li><a href="http://www.cyberciti.biz/tips/linux-procfs-file-descriptors.html">Linux: Find Out How Many File Descriptors Are Being Used</a></li>
<li><a href="http://www.unixmantra.com/2013/06/resource-limits-on-unix-systems-ulimit.html">Resource limits on UNIX systems (ulimit)</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bulk user creation in vBulletin 5 with CasperJS]]></title>
    <link href="http://pfigue.github.com/blog/2014/04/02/bulk-user-creation-in-vbulletin-5-with-casperjs/"/>
    <updated>2014-04-02T20:48:00+02:00</updated>
    <id>http://pfigue.github.com/blog/2014/04/02/bulk-user-creation-in-vbulletin-5-with-casperjs</id>
    <content type="html"><![CDATA[<h2>The Problem</h2>

<p>I had to create around 2000 users into a forum running with vBulletin 5.</p>

<p>I saw 3 options:</p>

<ul>
<li> Using vBulletin API, but for version 5 it is not yet documented at the time of writing this. One alternative is to reverse engineer it with help of version 4 docs., but this is not the fastest.</li>
<li> Accesing directly the database and creating there the needed rows, but I had no access to the database, and a little bit of reversing is probably needed to understand the dependences between the different tables.</li>
<li> Use PhantomJS/CasperJS/python-mechanize to manipulate the interface in an automated way. The interface had some Javascript, so python-mechanize would be excluded. CasperJS (a framework, running over PhantomJS) is the option I chose.</li>
</ul>


<h2>CasperJS</h2>

<p>I have never used it before, neither PhantomJS, so I didn&rsquo;t know how complicated it would be. <a href="http://docs.casperjs.org/en/latest/installation.html">Fortunately, it was quite easy and fast to setup</a>.</p>

<p>This is the script I used, which can be used as <code>casperjs script.js "newuser_username" "newuser_password" "newuser_email"</code></p>

<pre><code>var casper = require('casper').create();

casper.start('http://&lt;URL to the forum&gt;/admincp/', function() {
    this.echo(this.getTitle()); // Some logging to know if the script is running
    console.log(casper.cli.args[0])
    console.log(casper.cli.args[1])
    console.log(casper.cli.args[2])
    this.fill('form[action="../login.php?do=login"]',
        {
            'vb_login_username':'&lt;An Admin Username&gt;',
            'vb_login_password': '&lt;Admin\'s password&gt;,
    },
    true);
});

casper.thenOpen('http://&lt;URL to the forum&gt;/admincp/user.php?do=add&amp;', function(){
    this.echo(this.getTitle()); // Some logging to know if the script is running
    this.fill('form[action="user.php?do=update"]',
        {
            'user[username]': casper.cli.args[0],
            'password': casper.cli.args[1],
            'user[email]': casper.cli.args[2],
            'user[membergroupids][]': ['14', '12'],  // List of group numbers (actually, strings) the user belongs to.
        },
        true);
});

casper.then(function() {
    this.echo(this.getTitle());  // Some logging to know if the script is running
//    console.log(casper.cli.args[0])
});

casper.run();
</code></pre>

<h2>Results</h2>

<p>First I tried creating one user, then creating three, then all the 2000 users. It needs around 10 or 15 seconds per user, so it is not the fastest solution, but the simplest I had.</p>

<p>The script can be called several times with identical parameters and the result will be the same.</p>

<p>Running several instances in parallel can also be an alternative.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Creating a new profile for Lintian]]></title>
    <link href="http://pfigue.github.com/blog/2014/02/04/creating-a-new-profile-for-lintian/"/>
    <updated>2014-02-04T09:56:00+01:00</updated>
    <id>http://pfigue.github.com/blog/2014/02/04/creating-a-new-profile-for-lintian</id>
    <content type="html"><![CDATA[<p>Lintian is a tool to check the format of Debian Packages.</p>

<p>It is useful when you are building your own packages, to check if they are properly built before testing them by actually installing them (for example, <a href="http://pfigue.github.io/blog/2014/01/25/using-vagrant-with-lxc-linux-containers/">through Vagrant into fresh containers</a>).</p>

<p>It can use different profiles, with different checks. The default profile is <strong>debian/main</strong>, which is useful if you pretend to package a program for the official Debian repositories.</p>

<p>After installing <strong>Lintian</strong> with <code>sudo apt-get install lintian</code> in Ubuntu 12.04, I have a file <code>/usr/share/lintian/profiles/debian/main.profile</code>. That is the definition of the <em>debian/main</em> profile (or just <em>debian</em>).</p>

<p>Lintian looks for profiles in this order,</p>

<ul>
<li> first in the user&rsquo;s domain <code>$HOME/.lintian/profiles/</code></li>
<li> then in system&rsquo;s domain <code>/etc/lintian/profiles</code></li>
<li> and finally in <code>/ur/share/lintian/profiles</code>.</li>
</ul>


<p>So, what I did is creating a new profile, (filename ended in <em>.profile</em>):</p>

<pre><code>$ mkdir -p /home/pfigue/.lintian/acme_gmbh/my_cute_profile/
$ vim /home/pfigue/.lintian/acme_gmbh/my_cute_profile/main.profile
</code></pre>

<p>There, the content of the profile file in my case is:</p>

<pre><code>Profile: acme_gmbh/main
Extends: debian/main
Disable-Tags: dir-or-file-in-opt
</code></pre>

<p>Which actually inherits all the rules from debian/main, and disables the check to avoid installation in <code>/opt/</code> (dir-or-file-in-opt). That path is reserved for private software, not for Debian software, that&rsquo;s the reason why the Debian profile checks nothing is installed there.</p>

<p>Once the new profile is in place, you can check it with:</p>

<pre><code>$ lintian --profile acme_gmbh  /tmp/testapp_0.45_amd64.deb
</code></pre>

<p>Per default the profile <em>acme_gmbh</em> is an alias for <em>acme_gmbh/main</em>.</p>

<p>References:</p>

<ul>
<li> <a href="http://lintian.debian.org/manual/section-2.5.html">Lintian User&rsquo;s Manual: Vendor Profiles</a></li>
<li> <a href="http://pfigue.github.io/blog/2014/01/25/using-vagrant-with-lxc-linux-containers/">Using Vagrant with LXC Linux Containers</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Vagrant with LXC Linux Containers]]></title>
    <link href="http://pfigue.github.com/blog/2014/01/25/using-vagrant-with-lxc-linux-containers/"/>
    <updated>2014-01-25T15:44:00+01:00</updated>
    <id>http://pfigue.github.com/blog/2014/01/25/using-vagrant-with-lxc-linux-containers</id>
    <content type="html"><![CDATA[<h2>Containers</h2>

<p>LXC Containers are a relatively new way of virtualization for Linux environments. Solaris had containers since several years ago, but in Linux they appeared around 5 years ago, afaik.</p>

<p>The point of virtualizing using containers instead of virtual machines is that, both, guest and host, use the same instance of the kernel, which translates in <strong>far less time to boot and far less resources used</strong>.</p>

<p>A container is just a set of resources and processes running in a different namespace, eventually with some limitations (CPU or I/O usage, memory used, etc.).</p>

<p>Afair, there are/were 2 implementations of containers for linux: OpenVZ and LXC. Without being an expert in the area, I&rsquo;d say that LXC has much better maintenance nowadays than OpenVZ.</p>

<h2>LXC Containers in Ubuntu</h2>

<p>In Ubuntu 12.04 we can install LXC as usual with <code>sudo apt-get install lxc</code>. As root, we can list the containers we have in every state with <code>sudo lxc-list</code>.</p>

<p>Usually, <a href="https://help.ubuntu.com/lts/serverguide/lxc.html#lxcbr0">once <strong>lxc</strong> is installed, we should find a new network bridge</a>: <em>lxcbr0</em>. You can check if it is present with:</p>

<pre><code>$ ifconfig lxcbr0 2&gt; /dev/null 1&gt;&amp;2  &amp;&amp; echo 'lxc interface is present' || echo 'no lxc interface'
lxc interface is present
$
</code></pre>

<p>Before start to use containers with Vagrant, we should <a href="https://github.com/fgrehm/vagrant-lxc/wiki/Avoiding-%27sudo%27-passwords">work around a bug in sudo (Ubuntu 12.04)</a>.</p>

<p>With <code>sudo visudo</code> you can add the line:</p>

<pre><code>Defaults        !tty_tickets
</code></pre>

<p>to the sudoers file. It will solve the bug.</p>

<h2>Vagrant with the LXC provider</h2>

<p>Let&rsquo;s create a new vagrant project:</p>

<pre><code>$ mkdir vagrant-lxc
$ cd vagrant-lxc/
$ vagrant init test-lxc http://dl.dropbox.com/u/13510779/lxc-precise-amd64-2013-07-12.box
</code></pre>

<p>Where the URL is an <a href="http://dl.dropbox.com/u/13510779/lxc-precise-amd64-2013-07-12.box">Ubuntu 12.04 amd64 box <strong>for LXC</strong></a> from <a href="http://vagrantbox.es/">vagrantbox.es</a>.</p>

<p>You can use the same <a href="http://pfigue.github.io/blog/2014/01/25/my-first-steps-with-vagrant/">Vagrantfile I used in the previous article</a>, but notice that the <code>config.vm.network</code> parameter is ignored by this provider.
At the moment you can only forward port, but not bridge any interface.</p>

<p>Now you can <a href="https://github.com/fgrehm/vagrant-lxc/blob/master/README.md">install there the <strong>vagrant-lxc</strong> plugin</a> with <code>vagrant plugin install vagrant-lxc</code>.</p>

<p>Once done you should already be able to fire up that box with LXC:</p>

<pre><code>$ vagrant up --provider=lxc
[default] Warning! The LXC provider doesn't support any of the Vagrant public / private
network configurations (ex: `config.vm.network :private_network, ip: "some-ip"`).
They will be silently ignored.
</code></pre>

<p><code>vagrant ssh</code> from the same <code>vagrant-lxc/</code> directory to check it, or <code>sudo lxc-list</code> or <code>lxc-ps --lxc</code> to visualice the running containers.</p>

<h2>Advantages</h2>

<p>The time needed to boot up a Vagrant box with LXC is really much lower than using VirtualBox. This translates in faster try-error loop, faster feedback and <a href="http://pfigue.github.io/blog/2014/01/25/my-first-steps-with-vagrant/">faster development of Puppet Manifest, server configuration</a>, <a href="http://pfigue.github.io/blog/2014/01/25/my-first-steps-with-vagrant/">faster testing of packages</a>, etc.</p>

<p>References:</p>

<ul>
<li> <a href="https://help.ubuntu.com/lts/serverguide/lxc.html#lxcbr0">LXC &ndash; lxcbr0</a></li>
<li> <a href="https://github.com/fgrehm/vagrant-lxc/blob/master/README.md">README for vagrant-lxc plugin</a></li>
<li> <a href="https://github.com/fgrehm/vagrant-lxc/wiki/Avoiding-%27sudo%27-passwords">vagrant-lxc &ndash; BUG &ndash; Avoiding &lsquo;sudo&rsquo; passwords</a></li>
<li> <a href="http://pfigue.github.io/blog/2014/01/25/my-first-steps-with-vagrant/">My first steps with Vagrant</a></li>
<li> <a href="http://fabiorehm.com/blog/2013/04/28/lxc-provider-for-vagrant/">LXC provider for Vagrant</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[My first steps with Vagrant]]></title>
    <link href="http://pfigue.github.com/blog/2014/01/25/my-first-steps-with-vagrant/"/>
    <updated>2014-01-25T12:52:00+01:00</updated>
    <id>http://pfigue.github.com/blog/2014/01/25/my-first-steps-with-vagrant</id>
    <content type="html"><![CDATA[<h2>Why Vagrant?</h2>

<p>Lately I was developing my-own-solution &trade; for package deployment and now I&rsquo;d to check if I can install system packages without problems.</p>

<p><strong>Vagrant is a tool that lets me create, destroy and recreate Virtual Machines very easily</strong>, so I can in less than 5 minutes, create a new VM with a fresh install of Ubuntu 12.04 and install there the package I want to test.</p>

<p>It can also be useful to test system configuration (e.g. Puppet manifests or Chef recipes, nginx config., etc.) without playing with the production environment.</p>

<p>Another interesting use (and the reason I want to add Vagrant to the development toolchain my team uses at work) is to provide every developer with the option of packing his/her branch and deploy it with the help of Vagrant into a fresh VM, locally on his/her laptop.</p>

<p>Once deployed there, unit and functional test could be run in/against that VM, to be sure that every single branch sent to staging passed a minimal set of requirements.</p>

<h2>Replacing the old-Ubuntu-12.04 Vagrant install with a factory Vagrant.</h2>

<p>Long time ago I installed <strong>vagrant</strong> via <code>sudo apt-get install vagrant</code>, and since then I was getting an error whenever I tried to start a new project:</p>

<pre><code>$ vagrant init test1 http://cloud-images.ubuntu.com/vagrant/precise/current/precise-server-cloudimg-amd64-vagrant-disk1.box
/home/pablo/.rvm/rubies/ruby-2.0.0-p353/lib/ruby/site_ruby/2.0.0/rubygems/core_ext/kernel_require.rb:55:in `require': cannot load such file -- log4r (LoadError)
        from /home/pablo/.rvm/rubies/ruby-2.0.0-p353/lib/ruby/site_ruby/2.0.0/rubygems/core_ext/kernel_require.rb:55:in `require'
        from /usr/bin/vagrant:2:in `&lt;main&gt;'
</code></pre>

<p>I decided to remove the version of Vagrant shipped with Ubuntu 12.04, and install a new one from <a href="http://www.vagrantup.com/downloads.html">the debian package I could get via HashiCorp</a>:</p>

<pre><code>$ sudo apt-get remove --purge vagrant
$ wget "https://dl.bintray.com/mitchellh/vagrant/vagrant_1.4.3_x86_64.deb"
$ sudo dpkg -i vagrant_1.4.3_x86_64.deb
</code></pre>

<p>Due to the first installation of Vagrant, probably all the dependencies were already satisfied and I didn&rsquo;t have to install anything else.</p>

<h2>First project</h2>

<p>In Vagrant, you create a new directory for every project. In that directory will live a <code>Vagrantfile</code> with all the configuration of the VM(s) for that project.</p>

<pre><code>$ mkdir vagrant
$ cd vagrant/
$ vagrant init test1 http://cloud-images.ubuntu.com/vagrant/precise/current/precise-server-cloudimg-amd64-vagrant-disk1.box
A `Vagrantfile` has been placed in this directory. You are now
ready to `vagrant up` your first virtual environment! Please read
the comments in the Vagrantfile as well as documentation on
`vagrantup.com` for more information on using Vagrant.
</code></pre>

<p>If you are going to use Vagrant as a testing/deployment solution at work, you may find interesting to track the Vagrantfile via a <em>git</em> repository.</p>

<p>Let&rsquo;s edit the Vagrantfile a little bit to redirect the port 8080 on my laptop to the port 8000 in the test1 VM. The result would look like:</p>

<pre><code># Vagrantfile API/syntax version. Don't touch unless you know what you're doing!
VAGRANTFILE_API_VERSION = "2"

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
  config.vm.box = "test1"
  config.vm.box_url = "http://cloud-images.ubuntu.com/vagrant/precise/current/precise-server-cloudimg-amd64-vagrant-disk1.box"
  config.vm.network :forwarded_port, guest: 8000, host: 8080
end
</code></pre>

<p>The URL <a href="http://cloud-images.ubuntu.com/vagrant/precise/current/precise-server-cloudimg-amd64-vagrant-disk1.box">http://cloud-images.ubuntu.com/vagrant/precise/current/precise-server-cloudimg-amd64-vagrant-disk1.box</a> is a boxed <a href="http://cloud-images.ubuntu.com/vagrant/precise/current/precise-server-cloudimg-amd64-vagrant-disk1.box">Ubuntu 12.04 amd64 (for VirtualBox)</a> that I found in <a href="http://www.vagantbox.es">vagrantbox.es</a>.</p>

<p>Once the file is edited and saved, if now I run <code>vagrant up</code>, as it is the first time I fire that VM up, it will fetch the .box file (around 400M big) and create the VM.</p>

<p>Per default, the virtualization <em>provider</em> is VirtualBox, but I&rsquo;ve seen boxes for VMWare and for Linux Containers (LXC).</p>

<h2>Testing the port redirect</h2>

<p>Typing <code>vagrant ssh</code> in the shell inside the <code>vagrant/</code> directory will establish an SSH session with the VM.</p>

<p>Once there you can type this to get netcat listening on port 8000 of the VM:</p>

<pre><code>$ nc -l 0.0.0.0 8000
</code></pre>

<p>In other terminal you can try to access the port 8080 of your computer (the host of the VM), like:</p>

<pre><code>$ curl http://127.0.0.1:8080
</code></pre>

<p>It should redirect the packets from 8080/tcp in the host to 8000/tcp in the VM, and you should be able to see the HTTP request in the terminal where netcat is running.</p>

<h2>Trying a network bridge instead of a port redirect</h2>

<p>If the VM will need to access a computer in the same network as the host (e.g. a database server), or the VM will be accessed by a computer different than the host (e.g. a <a href="http://code.google.com/p/selenium/wiki/Grid2">Selenium Grid</a>), you may find easier to install a network bridge better than redirect some ports.</p>

<p>With that purpose in mind, the Vagrantfile should be something like:</p>

<pre><code># Vagrantfile API/syntax version. Don't touch unless you know what you're doing!
VAGRANTFILE_API_VERSION = "2"

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
  config.vm.box = "test1"
  config.vm.box_url = "http://cloud-images.ubuntu.com/vagrant/precise/current/precise-server-cloudimg-amd64-vagrant-disk1.box"
  config.vm.network :public_network
end
</code></pre>

<p>The new Vagrant configuration can be applied with <code>vagrant reload</code>. That command will stop (in a polite and slow way) and start again the VMs, asking this time the user in the command-line for which network interface will be the VM bridged to.</p>

<p>Once the VM is completelly booted, from a <code>vagrant ssh</code> session, the available interfaces can be checked:</p>

<pre><code>$ ifconfig -a | perl -ne 'print "$1\n" if m/^(\S+)/'
eth0
eth1
lo
$ ifconfig eth1 | grep inet\ addr
          inet addr:192.168.0.105  Bcast:192.168.0.255  Mask:255.255.255.0
</code></pre>

<p>So, my VM got the IP address 192.168.0.105. If I start a netcat in port 90/tcp with <code>sudo nc -l 0.0.0.0 90</code></p>

<p>Then, I should be able to connect to port 90/tcp of the VM from my laptop, with something like <code>telnet 192.168.0.105 90</code>.</p>

<p>Once you stop playing with Vagrant, you can stop the VM via <code>vagrant halt</code>.</p>

<p>References:</p>

<ul>
<li><p> <a href="http://www.jedi.be/blog/2011/03/28/using-vagrant-as-a-team/">Using Vagrant as a Team &ndash; Just Enough Developed Infrastructure</a></p></li>
<li><p> <a href="http://www.vagrantbox.es/">A list of base boxes for Vagrant</a></p></li>
<li><p> <a href="http://www.vagrantup.com/downloads.html">Vagrant Packages and Tarballs</a></p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Configuring dnsmasq to serve my own domain name zone]]></title>
    <link href="http://pfigue.github.com/blog/2013/12/06/configuring-dnsmasq-to-serve-my-own-domain-name-zone/"/>
    <updated>2013-12-06T17:07:00+01:00</updated>
    <id>http://pfigue.github.com/blog/2013/12/06/configuring-dnsmasq-to-serve-my-own-domain-name-zone</id>
    <content type="html"><![CDATA[<h1>Why a name server and why dnsmasq.</h1>

<p>I have a datacenter (Acme GmbH.), with several environments (green, blue, live, staging), each environment with several servers (server1, server2, &hellip; serverX) and I want to use a domain name service to name every server on the datacenter, so every server can easily know what is the IP of server3.staging.acme, for example.</p>

<p>I may want also to create some aliases like puppet-master.acme.</p>

<p>While I could do this with BIND, it actually seems easier to me to user <em>dnsmasq</em>.</p>

<p>dnsmasq is a small DNS server easy to configure. It has also DHCP integration (and TFTP, PXE, etc.) but it is not what I am looking for.</p>

<p>As alternatives to use my own DNS server, I could choose some setup with Route 53 as user of Amazon. Digital Ocean or other providers may have their own systems.</p>

<h1>Installing</h1>

<p>In Ubuntu 12.04 it is just <code>sudo apt-get install dnsmasq</code>. It will launch the service, that you can stop via <code>/etc/init.d/dnsmasq stop</code>.</p>

<p>The configuration file is in <code>/etc/dnsmasq.conf</code>. The files in <code>/etc/dnsmasq.d/</code> can be included from dnsmasq.conf, but per default they aren&rsquo;t.</p>

<h1>Testing and finding the right configuration</h1>

<p>The <em>&mdash;no-daemon</em> option will be useful to do test in the command line:</p>

<pre><code>/usr/sbin/dnsmasq --no-daemon
</code></pre>

<p><em>&mdash;no-hosts</em> and <em>&mdash;addn-hosts=/etc/hosts.acme</em> will prevent dnsmasq to read /etc/hosts and instead load the datacenter zone configuration from <code>/etc/hosts.acme</code>:</p>

<pre><code>/usr/sbin/dnsmasq --no-daemon --no-hosts --addn-hosts=/etc/hosts.acme
</code></pre>

<p><em>&mdash;interface=eth0</em> will force dnsmasq to listen only on that interface. In my case, this will run in an AWS EC2 Instance, so I can configure in the Amazon Web Console the Security Groups to firewall the service and let only the legit servers to access the zone entries instead of everybody in the Amazon Datacenter network.</p>

<p><em>&mdash;no-resolv</em> will avoid dnsmasq to read the list of name servers from <code>/etc/resolv.conf</code> and <em>&mdash;server=&ldquo;8.8.4.4&rdquo;</em> will provide a default one, in this case a Google nameserver</p>

<pre><code>/usr/sbin/dnsmasq --no-daemon --no-hosts --addn-hosts=/etc/hosts.acme --interface=eth0 --no-resolv --server="8.8.4.4"
</code></pre>

<h1>Setting up the DNS server</h1>

<p>Instead of launching the command manually, we will launch the service via <code>/etc/init.d/dnsmasq start|status|stop</code>. The default configuration is <code>/etc/dnsmasq.conf</code> and I have this there:</p>

<pre><code>no-resolv
server=8.8.4.4
server=8.8.8.8
interface=eth0
no-dhcp-interface=eth0
no-hosts
addn-hosts=/etc/hosts.acme
log-queries
log-dhcp
</code></pre>

<p>The <strong>user</strong> and <strong>group</strong> directives are not needed in Ubuntu, as the server is run always ad <strong>dnsmasq</strong> user. Only during testing we will use <strong>log-queries</strong> and <strong>log-dhcp</strong>, to see what is going on. Once finished the tests, we can remove them.</p>

<p>In <code>/etc/hosts.acme</code> just 1 entry (if you are copying tis, be sure of using a valid IP address, with numbers, not that z.z.z.z):</p>

<pre><code>z.z.z.z     server2.staging.acme
</code></pre>

<p>If I launch the service like <code>/usr/sbin/dnsmasq -C /etc/dnsmasq.conf --no-daemon</code> (as root) I get this:</p>

<pre><code>dnsmasq: started, version 2.59 cachesize 150
dnsmasq: compile time options: IPv6 GNU-getopt DBus i18n DHCP TFTP conntrack IDN
dnsmasq: using nameserver 8.8.8.8#53
dnsmasq: using nameserver 8.8.4.4#53
dnsmasq: read /etc/hosts.acme - 1 addresses
</code></pre>

<p>Then, from other server (IP: y.y.y.y) with access to port 53/udp of the server (name: server1.acme, IP: x.x.x.x) where dnsmasq is running, I do:</p>

<pre><code># dig @server1.acme www.google.de

; &lt;&lt;&gt;&gt; DiG 9.8.1-P1 &lt;&lt;&gt;&gt; @server1.acme www.google.de
; (1 server found)
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 43199
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
;www.google.de.                 IN      A

;; ANSWER SECTION:
www.google.de.          300     IN      A       74.125.24.94

;; Query time: 21 msec
;; SERVER: x.x.x.x#53(x.x.x.x)
;; WHEN: Tue Dec  3 16:35:58 2013
;; MSG SIZE  rcvd: 47
</code></pre>

<p>It seems the name server can resolve external domain names. And actually I see in the server:</p>

<pre><code>dnsmasq: query[A] www.google.de from y.y.y.y
dnsmasq: forwarded www.google.de to 8.8.4.4
dnsmasq: forwarded www.google.de to 8.8.8.8
dnsmasq: reply www.google.de is 74.125.24.94
</code></pre>

<p>So, it seems to be forwarding my requests to google&rsquo;s DNSs (8.8.4.4 and 8.8.8.8) as they are not in the cache of dnsmasq.</p>

<p>If I ask again (2 seconds later) for the same external name:</p>

<pre><code>dig @server1.acme www.google.de

; &lt;&lt;&gt;&gt; DiG 9.8.1-P1 &lt;&lt;&gt;&gt; @server1.acme www.google.de
; (1 server found)
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 36570
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
;www.google.de.                 IN      A

;; ANSWER SECTION:
www.google.de.          298     IN      A       74.125.24.94

;; Query time: 2 msec
;; SERVER: x.x.x.x#53(x.x.x.x)
;; WHEN: Tue Dec  3 16:36:00 2013
;; MSG SIZE  rcvd: 47
</code></pre>

<p>The answer I get has a TTL 2 seconds lower and in the server I see:</p>

<pre><code>dnsmasq: query[A] www.google.de from y.y.y.y
dnsmasq: cached www.google.de is 74.125.24.94
</code></pre>

<p>This time dnsmasq had the answer itself and didn&rsquo;t need to forward. In fact the query time went from 21msec to 2msec.</p>

<p>If I ask for an internal name:</p>

<pre><code>dig @server1.acme server2.staging.acme

; &lt;&lt;&gt;&gt; DiG 9.8.1-P1 &lt;&lt;&gt;&gt; @server1.acme server2.staging.acme;
; (1 server found)
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 54617
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
;server2.staging.acme.           IN      A

;; ANSWER SECTION:
server2.staging.acme.    0       IN      A       z.z.z.z

;; Query time: 2 msec
;; SERVER: x.x.x.x#53(x.x.x.x)
;; WHEN: Tue Dec  3 16:36:13 2013
;; MSG SIZE  rcvd: 53
</code></pre>

<p>I get this in the server:</p>

<pre><code>dnsmasq: query[A] server2.staging.acme from y.y.y.y
dnsmasq: /etc/hosts.acme server2.staging.acme is z.z.z.z
</code></pre>

<p>The server is also able to solve internal names.</p>

<h1>Automatising the installation with Puppet</h1>

<p>I have a <a href="https://gist.github.com/pfigue/7772289">Gist with an draft of a manifest</a> for Puppet.</p>

<p>Use <code>puppet apply --noop dnsmasq.pp</code> to check it (eventually w/o &mdash;noop).</p>

<h1>References:</h1>

<ul>
<li><a href="http://www.thekelleys.org.uk/dnsmasq/docs/dnsmasq-man.html">dnsmasq manual page</a></li>
<li><a href="http://en.wikipedia.org/wiki/Comparison_of_DNS_server_software">Comparison of DNS Server Software</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Replacing DHCP automatic name resolution config with my own static one]]></title>
    <link href="http://pfigue.github.com/blog/2013/12/04/replacing-dhcp-automatic-name-resolution-config-with-my-own-static-one/"/>
    <updated>2013-12-04T13:07:00+01:00</updated>
    <id>http://pfigue.github.com/blog/2013/12/04/replacing-dhcp-automatic-name-resolution-config-with-my-own-static-one</id>
    <content type="html"><![CDATA[<p><em>Problem1:</em> I have an AWS EC2 Instance (Ubuntu 12.04) where I want to use <a href="http://pfigue.github.io/blog/2013/12/06/configuring-dnsmasq-to-serve-my-own-domain-name-zone/">my own DNS service</a>, not the default one provided by Amazon.</p>

<p><em>Solution1:</em> Definining a static DNS server configuration</p>

<p>In <code>/etc/network/interfaces</code> you can specify <strong>dns-search</strong> and <strong>dns-nameservers</strong> in the configuration of the network interface,</p>

<pre><code># The primary network interface
auto eth0
iface eth0 inet dhcp
        dns-search acme
        dns-nameservers 1.2.3.4
</code></pre>

<p>Now if you apply this configuration with <code>/etc/init.d/networking restart</code>, you will be able to see changes in <code>/etc/resolv.conf</code>.</p>

<p><em>Note1:</em> Restarting the network services didn&rsquo;t disconnect me from the computer I was configuring.</p>

<p><em>Note2:</em> There may be better ways to apply changes in the network config.</p>

<p><em>Problem2:</em> In /etc/resolv.conf there is my static configuration <strong>together with the DNS setup received via DHCP</strong>, and I want only my configuration without the one provided by DHCP.</p>

<p><em>Solution2:</em> Avoid the DNS setup from DHCP.</p>

<p>My system is using dhclient3, and the config. file for it is in <code>/etc/dhcp/dhclient.conf</code>. There, I comment the options to retrieve the DNS config from the DHCP service:</p>

<pre><code>request subnet-mask, broadcast-address, time-offset, routers,
        #domain-name, domain-name-servers, domain-search, host-name,
        netbios-name-servers, netbios-scope, interface-mtu,
        rfc3442-classless-static-routes, ntp-servers,
</code></pre>

<p>Now if I restart the <code>networking</code> service again, <code>/etc/resolv.conf</code> will have the right domain and nameservers.</p>

<p>References:</p>

<ul>
<li><a href="http://ubuntuforums.org/showthread.php?t=1320773">Static DNS with DHCP</a></li>
<li><a href="https://help.ubuntu.com/12.04/serverguide/network-configuration.html#name-resolution">Ubuntu 12.04: Network Configuration: Name Resolution</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Puppet Manifest to set the timezone]]></title>
    <link href="http://pfigue.github.com/blog/2013/11/29/a-puppet-manifest-to-set-the-timezone/"/>
    <updated>2013-11-29T16:48:00+01:00</updated>
    <id>http://pfigue.github.com/blog/2013/11/29/a-puppet-manifest-to-set-the-timezone</id>
    <content type="html"><![CDATA[<p>I am getting to know Puppet and right now I would like to set the timezone of my servers in an automated way via Puppet Manifests.</p>

<h1>Ensuring the presence and content of a file</h1>

<p>I created a new file <code>timezone.pp</code> and wrote inside:</p>

<pre><code>file { '/etc/timezone':
    ensure =&gt; present,
    content =&gt; "Europe/Berlin\n",
}
</code></pre>

<p>Saved it, and run <code>puppet apply --noop timezone.pp</code> as root. The <em>&mdash;noop</em> is to indicate puppet that I want a dry-run, to not make any changes into the system, just validate the Manifest.</p>

<p>Those 4 lines define a <a href="http://docs.puppetlabs.com/references/latest/type.html#file">ressource type file</a>, the name of the ressource and (in this case) also the path of the file is <em>/etc/timezone</em>, the file should be present into the filesystem and the content should be the indicated one.</p>

<p>With the file ressource I could also define symbolic links, directories, etc.</p>

<p>Once the validation with <code>puppet apply --noop</code> was okay, I moved to the next step.</p>

<h1>Running an external command</h1>

<p>With the <em>exec</em> ressource I can run a command as a given user. The command should be expressed as an absolute path, so <code>/usr/sbin/dpkg-reconfigure</code> instead of <code>dpkg-reconfigure</code>:</p>

<pre><code>exec { 'reconfigure-tzdata':
        user =&gt; root,
        group =&gt; root,
        command =&gt; '/usr/sbin/dpkg-reconfigure --frontend noninteractive tzdata',
}
</code></pre>

<p>Also note that the <a href="https://help.ubuntu.com/community/UbuntuTime#Using_the_Command_Line_.28unattended.29">command won&rsquo;t require input from the user</a> and it will be run as root.</p>

<h1>Reporting and being talkative</h1>

<p>To keep the user updated and make the debugging easier, I would like to notify the user what is going on:</p>

<pre><code>notify { 'timezone-changed':
        message =&gt; 'Timezone was updated to Europe/Berlin',
}
</code></pre>

<p>This ressource will be named <code>timezone-changed</code> and represents a notification message. There are also fail{} and notice{} ressources, afaik.</p>

<h1>Execution order</h1>

<p>Puppet Manifests are declarative. You just write down statement of what do you want to get, not how to get it.</p>

<p>Now I want to express that the <em>timezone-changed notify ressource</em> should be executed only after the <em>reconfigure-tzdata exec ressource</em>. And this one should be executed only after <em>/etc/timezone file ressource</em>:</p>

<pre><code>File['/etc/timezone'] -&gt; Exec['reconfigure-tzdata'] -&gt; Notify['timezone-changed']
</code></pre>

<p>So, if /etc/timezone already exists and has the right contents and (in general) meets the specification of the ressource we wrote in the manifest, no reconfigure-tzdata will be executed and no notification will happen.</p>

<h1>Applying the changes</h1>

<p>Having all the previous ressources and ordering in <a href="https://gist.github.com/pfigue/7694021">timezone.pp</a>, as root I run <code>puppet apply timezone.pp</code>:</p>

<pre><code>notice: /Stage[main]//File[/etc/timezone]/content: content changed '{md5}4f24b133ba38d8fd565168742c9aedeb' to '{md5}749357f70f40574f632071ec7d5f41a9'
notice: /Stage[main]//Exec[reconfigure-tzdata]/returns: executed successfully
notice: Timezone was updated to Europe/Berlin
notice: /Stage[main]//Notify[timezone-changed]/message: defined 'message' as 'Timezone was updated to Europe/Berlin'
notice: Finished catalog run in 1.16 seconds
</code></pre>

<p>Checking:</p>

<pre><code># date
Thu Nov 28 16:10:51 CET 2013
#
</code></pre>

<p>Time zone is now <em>CET</em> which is Berlin, which is right.</p>

<h1>References:</h1>

<ul>
<li><a href="https://help.ubuntu.com/community/UbuntuTime#Using_the_Command_Line_.28unattended.29">Unattended timezone change in Ubuntu</a></li>
<li><a href="http://docs.puppetlabs.com/references/latest/type.html">List of Ressources for Puppet</a></li>
<li><a href="http://docs.puppetlabs.com/learning/ordering.html">Ressource Ordering in Puppet</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A custom Facter fact to read AWS User-Data]]></title>
    <link href="http://pfigue.github.com/blog/2013/11/28/a-custom-facter-fact-to-read-aws-user-data/"/>
    <updated>2013-11-28T12:35:00+01:00</updated>
    <id>http://pfigue.github.com/blog/2013/11/28/a-custom-facter-fact-to-read-aws-user-data</id>
    <content type="html"><![CDATA[<h2>I need to do the provisioning of some AWS EC2 Instances depending on their role.</h2>

<p>Some of them will need to install some packages with some configuration, others will need different packages and different configuration, depending on the role they will adopt in the new system.</p>

<p>For this reason I need Puppet to behave in a different way depending on a <strong>fact</strong>, the <em>server flavour</em> of the server who is being provisioned.</p>

<h2>How to communicate the role to Puppet</h2>

<p>The first exercise would be to learn how <a href="http://pfigue.github.io/blog/2013/11/27/how-to-add-custom-facts-to-facter/">to write a new dummy fact for Facter</a>, so we are sure we can write facts for Puppet.</p>

<p>The second goal would be to write a new fact that actually reads the role of the server.</p>

<h3>Appending custom information to an EC2 instance.</h3>

<p>I communicate the role (and other information) to a new server <strong>via User-Data</strong>, whenever I <em>Launch a New Instance</em> from the AWS Web Console.
During the 3rd step, named <em>Configure Instance Details</em>, I have the option to define <em>Advanced Details</em>, one of them the <em>User data</em>.</p>

<p>Once the instance is running, I <strong>can&rsquo;t change that User-Data</strong>. If I want to, I need to destroy that instance and create a new one with the new data.</p>

<p>An <strong>alternative to User-Data is to use <em>Tags</em></strong>, and then the EC2 Command-Line Tools to access and read information from those Tags. But you should mind that there is a <strong>limit of 10 tags per server</strong>, so it may not be the best solution. On the other hand, <strong>you can change the tags while the Instance is running</strong>. No need to destroy it.</p>

<p>I upload a YAML file with all the information, and then, once the new EC2 Instance is up, from the instance itself I can get that YAML file:</p>

<pre><code>$ /usr/bin/curl --silent "http://169.254.169.254/latest/user-data/"
instance:
    flavor: application-server
$
</code></pre>

<p>With <a href="https://gist.github.com/pfigue/7690424">some python scripting like</a>:</p>

<pre><code>import sys
import yaml

if __name__ == '__main__':
    contents = sys.stdin.read()
    document = yaml.load(contents)
    for key in sys.argv[1:]:
            try:
                    document = document[key]
            except KeyError:
                    sys.exit(1)
    sys.stdout.write(str(document))
    sys.stdout.flush()
    sys.exit(0)
</code></pre>

<p>I can provide the script the YAML via stdin and then ask for some data:</p>

<pre><code>$ curl --silent "http://169.254.169.254/latest/user-data/" | python ./ec2-user-data-parser.py instance flavor
application-server
$
</code></pre>

<h3>Writting the Facter <em>fact</em></h3>

<p>Writting a new fact is now straight forward (I saved the python script in <code>/root/tools/ec2-user-data-parser.py</code>):</p>

<pre><code># server_flavor.rb

Facter.add("server_flavor") do
  setcode do
    Facter::Util::Resolution.exec('/usr/bin/curl --silent "http://169.254.169.254/latest/user-data/" | /usr/bin/python /root/tools/ec2-user-data-parser.py instance flavor')
  end
end
</code></pre>

<p>I save the code to <code>/root/facter/server_flavor.rb</code> and then test it:</p>

<pre><code>$ FACTERLIB=/root/facter/ facter server_flavor
application-server
$
</code></pre>

<p>I may need still to add some <code>export FACTERLIB=/root/facter/</code> to <code>/root/.bashrc</code>, but now my puppet manifests will know which kind of Amazon servers they are provisioning.</p>

<h2>References:</h2>

<ul>
<li><a href="https://gist.github.com/pfigue/7690424">The code in a Gist</a></li>
<li><a href="http://docs.puppetlabs.com/guides/custom_facts.html">Puppet Docs: Custom Facts</a></li>
<li><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AESDG-chapter-instancedata.html">AWS Instance Meta- and User- Data</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to add custom facts to Facter]]></title>
    <link href="http://pfigue.github.com/blog/2013/11/27/how-to-add-custom-facts-to-facter/"/>
    <updated>2013-11-27T18:55:00+01:00</updated>
    <id>http://pfigue.github.com/blog/2013/11/27/how-to-add-custom-facts-to-facter</id>
    <content type="html"><![CDATA[<h2>What is Facter?</h2>

<p>It is a Ruby tool that informs you about facts.</p>

<p>As far as I know, it is used often from Puppet, to know about properties of the environment you are going to provision.</p>

<p>Those properties can be things like <em>Operating System</em>, <em>Version of the kernel</em>, <em>Architecture</em>, <em>Amazon EC2 Instance ID</em>, <em>Available memory</em>, <em>Number of processors</em>, etc.</p>

<p>The <code>facter</code> tool by default has some values like those above: you can run <code>facter</code> in the shell to get a list of <em>facts</em>.</p>

<p>From a Puppet Manifest you could write statements dependent on those facts.
For example, to install some package when the manifest is applied to a Debian system, and install a different package when applied to a Red Hat one.</p>

<h2>Do you need to define your own Facts?</h2>

<p><strong>Facter</strong> looks for the <em>FACTERLIB</em> environment variable, which may point to a directory, if it is defined.</p>

<p>If you put in that directory a file named <code>hardware_platform.rb</code> (note the <em>.rb</em> extension), and inside you define:</p>

<pre><code># hardware_platform.rb

Facter.add("hardware_platform") do
    setcode do
        Facter::Util::Resolution.exec('/bin/uname -i')
    end
end
</code></pre>

<p>There will appear a new <em>Fact</em> called <em>hardware_platform</em>:</p>

<pre><code># facter | grep -i platform
hardware_platform =&gt; x86_64
#
</code></pre>

<p>You can invoke it also via <code>facter hardware_platform</code>:</p>

<pre><code># facter hardware_platform
x86_64
#
</code></pre>

<p>Pay attention, that the file name and the Fact name are the same. An the file extension is <em>.rb</em>.</p>

<p>In this way, you can have your own facts for your Puppet manifests.</p>

<p>Puppet Docs has much <a href="http://docs.puppetlabs.com/guides/custom_facts.html">more information</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Versioning APIs: do or don't]]></title>
    <link href="http://pfigue.github.com/blog/2013/09/24/versioning-apis-do-or-dont/"/>
    <updated>2013-09-24T00:27:00+02:00</updated>
    <id>http://pfigue.github.com/blog/2013/09/24/versioning-apis-do-or-dont</id>
    <content type="html"><![CDATA[<p>Today I visited <a href="https://secure.trifork.com/berlin-2013/freeevent/index.jsp?eventOID=5783">REST Beyond the Intro Level</a>, where <a href="http://www.innoq.com/blog/st/">Stefan Tilkov</a> explained several concepts regarding REST APIs.</p>

<p>Among other things he mentioned NOT to version APIs, and he took as argument the <a href="http://en.wikipedia.org/wiki/Robustness_principle">Postel&rsquo;s Law</a>:</p>

<pre><code>Be conservative in what you send, be liberal in what you accept
</code></pre>

<p>Keep your API always backwards compatible. Keep te same behaviour for your ressources, and if you can&rsquo;t keep it, create new ressources.</p>

<p>His idea is that the part of the system serving the API should be <em>conservative</em>, and should avoid <em>semantic jumps</em> when developers deploy new features. At the same time, the consumers of the API should be tolerant when they use it.</p>

<h2>Stefan, I disagree</h2>

<p>If Stefan has a point when he says that an API is always the same and it has the same essential behaviour (e.g. behind the <a href="https://dev.twitter.com/docs/api">Twitter REST API</a> is the same idea, independently of the version: essentially, provide access to read and post tweets.), personally I don&rsquo;t agree with him too much.</p>

<p>I think an API evolves a lot during its life and needs proper versioning. At least, the non-backwards compatible changes should be identified.</p>

<p>In a mature API, well designed, well and long tested, maybe there are no big changes. But in a <strong>new API, built for a not-very-well defined product, change is the constant, and non-backwards-compatible changes happen</strong>.</p>

<p>Also there could be plenty of different clients for an API, each of them programmed for a different version and expecting an specific behaviour. They will break if we don&rsquo;t serve different versions of the API.</p>

<ul>
<li>One alternative is put the version number in the URL, like <a href="http://api.v1.foo.bar/,">http://api.v1.foo.bar/,</a> or <a href="http://api.foo.bar/v1/">http://api.foo.bar/v1/</a></li>
<li>Another alternative (I like it more) it passing an argument with the version, like <a href="http://api.foo.bar/?v=1,">http://api.foo.bar/?v=1,</a> and default to the last stable version if no version number is provided.</li>
</ul>


<p>My work experience is <em>startup</em>, and there you don&rsquo;t know always which product are you developing. Decissions change often quite fast and you can&rsquo;t design for the long-term. Maybe if I had work in a more stable environment I had a different opinion and could easily agree with Stefan.</p>

<h2>Semantic Versioning</h2>

<p><a href="http://semver.org/">Semantic Versioning</a> is a nice schema to start: Major version number for uncompatible changes, Minor version for new features and Patch number for bugfixes.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Monitoring software]]></title>
    <link href="http://pfigue.github.com/blog/2013/09/21/monitoring-software/"/>
    <updated>2013-09-21T23:59:00+02:00</updated>
    <id>http://pfigue.github.com/blog/2013/09/21/monitoring-software</id>
    <content type="html"><![CDATA[<ul>
<li>Pandora FMS</li>
<li>Icinga/Nadios</li>
<li>Riemann</li>
<li>Splunk</li>
<li>Munin</li>
<li>Cacti</li>
<li>New Relic</li>
<li>Zabbix</li>
<li>OpenNMS</li>
<li>Ganglia</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deploying static pages]]></title>
    <link href="http://pfigue.github.com/blog/2013/08/19/deploying-static-pages/"/>
    <updated>2013-08-19T19:16:00+02:00</updated>
    <id>http://pfigue.github.com/blog/2013/08/19/deploying-static-pages</id>
    <content type="html"><![CDATA[<p>A external designer prepares some static pages as landing pages for a platform.</p>

<ul>
<li>They are stored in a github repository.</li>
<li>We should provide a comfortable way of deploying them to production.</li>
<li>Obviously, the external desginer is not allowed to access the production environment.</li>
</ul>


<p>Solution:</p>

<ul>
<li>we will provide github access to the repository, so the desginer will push/pull directly to github the change she does.</li>
<li>we will write a deployment script to fetch the last changes from github and put them in production</li>
</ul>


<h2>The deployment script</h2>

<p>Something very easy in shell script: fetch the last changes, clean the filetree and copy it to production</p>

<pre><code>#!/bin/bash

GIT_COPY=/opt/acme/repo-static-pages/
TARGET=/usr/share/nginx/www/
TMP_DIR=/tmp/repo-static-pages/


### Update local copy
pushd $GIT_COPY &gt; /dev/null
git fetch upstream
git checkout master
git pull upstream master
popd &gt; /dev/null

### Put to a temporary location and clean it
rm -rf $TMP_DIR
mkdir -p $TMP_DIR
cp -r $GIT_COPY* $TMP_DIR
find $TMP_DIR -iname ".git" -type d -exec rm -rf {} \;
find $TMP_DIR -iname "*.tar.gz" -type f -delete
find $TMP_DIR -iname "*.tar" -type f -delete

### Deploy
rm -rf $TARGET
mkdir $TARGET
cp -r $TMP_DIR* $TARGET

rm -rf $TMP_DIR 
</code></pre>

<p>Note that it will deploy the <em>master</em> branch.</p>

<p>Also, we have to prepare the environment:</p>

<ul>
<li>Create SSH keys that will be used as <strong>deployment keys</strong> for the repository: <code>ssh-keygen -t rsa</code>

<ul>
<li> Without password to be able to automatise the process.</li>
<li> Or with password and store the unencrypted key in an SSH keyring.</li>
</ul>
</li>
<li><code>mkdir -p /opt/acme/repo-static-pages/</code></li>
<li><code>git clone git@github.com:acme/repo-static-pages.git /opt/acme/repo-static-pages/</code></li>
<li><code>git remote add upstream git@github.com:acme/repo-static-pages.git</code></li>
</ul>


<p>Note that the <em>upstream</em> should use the SSH endpoint instead the HTTPS one, to be able to authentify via the <em>deployment key</em>.</p>

<h2>Deploying</h2>

<p>Alternatives:</p>

<ul>
<li>deploy manually on demand, but I would prefer to spend the time in something more interesting</li>
<li>Install a cronjob that weekly, daily or every 5 minutes will deploy the last changes in github.</li>
<li>Use a githook that will run the deployment when a Pull Request is merged into the master branch of the repo.</li>
</ul>


<p>In any case, we have to be sure that no several deployments are running concurrently.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Adjusting the locale settings]]></title>
    <link href="http://pfigue.github.com/blog/2013/08/15/adjusting-the-locale-settings/"/>
    <updated>2013-08-15T15:48:00+02:00</updated>
    <id>http://pfigue.github.com/blog/2013/08/15/adjusting-the-locale-settings</id>
    <content type="html"><![CDATA[<p>This morning we had a problem at work. Our django webapp was raising an <strong>UnicodeError</strong> exception because it was trying to access a file, whose filename was written with ciryllic characters.</p>

<p>The behaviour of python with unicode depends on the system locale, which is defined by <em>environment variables</em> like <strong>LC_ALL</strong>, <strong>LANG</strong>, <strong>LC_TYPE</strong>, etc.</p>

<p>In this case, the solution of the problem was first, to use a locale with <strong>UTF-8</strong>. But actually, the Upstart task that launches the <em>uwsgi</em> for the django app is providing several environment variables. Among others, <em>LC_ALL</em> and <em>LANG</em>, which refer to <em>de_DE.UTF-8</em> locale.</p>

<p>Till here, everything is right. Then, why is it not working?</p>

<p>Well, because the locale needs to be installed. Or more exactly, it needs to be generated:</p>

<pre><code>$ locale-gen de_DE.UTF-8
</code></pre>

<p>Apparently, <code>dpkg-reconfigure locales</code> checks for the locales and compiles them if necesary. For some yet-unknown-for-me reason, that locale was not being compiled by <em>dpkg-reconfigure</em> till I ran <em>locale-gen</em>.</p>

<p>Once this locales was compiled we didn&rsquo;t even need to restart the uwsgis to fix the application, though we did, just in case.</p>

<p><em>UnicodeError</em> went away and we won a karma cookie for the happy DevOps Team.</p>

<h2>Setting the right locale for the command line</h2>

<p>One thing is the locale for the django app, defined in the environment variables the python interpreter gets to run the uwsgi (i.e. defined in the Upstart task).</p>

<p>And other thing is the command line the operators use to do their stuff on the system. This one should be in English, or operators will break. To achieve this, <strong>LANG=en_US.UTF-8</strong>, for example. And the right way is to define it in <code>/etc/environment</code>. I defined also <strong>LC_ALL</strong> with the same value.</p>

<p>Now when running <code>dpkg-reconfigure locales</code> I don&rsquo;t get anymore the warnings I was getting before due to the lack of <em>LANG</em> and <em>LC_ALL</em> definition.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running Django administrative commands as cronjobs]]></title>
    <link href="http://pfigue.github.com/blog/2013/08/14/running-django-administrative-commands-as-cronjobs/"/>
    <updated>2013-08-14T18:17:00+02:00</updated>
    <id>http://pfigue.github.com/blog/2013/08/14/running-django-administrative-commands-as-cronjobs</id>
    <content type="html"><![CDATA[<p>My best practices (at the moment) to install periodic django administrative commands.</p>

<h1>Crontabs</h1>

<p>First of all, there are three crontabs:</p>

<ul>
<li> the system one (<em>/etc/crontab</em>)</li>
<li> and the user&rsquo;s one (<em>/var/spool/cron/crontabs/root</em>, e.g.).</li>
<li> those in <em>/etc/cron.d/</em>. Newly installed packages may want to install their cronjobs there.</li>
</ul>


<p>The syntax is different:</p>

<ul>
<li> in the system crontab (and /etc/cron.d) you indicate which user runs the cronjob.</li>
<li> while in the user&rsquo;s cronjob it is the user itself who runs the cronjobs.</li>
</ul>


<p>And the way to manipulate the files as well:</p>

<ul>
<li> system crontab is manipulated editing directly <em>/etc/crontab</em> or <em>/etc/cron.d/file</em>. Usually it is a script (like a <em>postinstall script</em> in a package) who updates it.</li>
<li><p> user&rsquo;s crontab is manipulated via the <code>crontab</code> command:</p>

<ul>
<li><code>crontab -e</code> to edit. <code>export EDITOR=/usr/bin/vim</code> may help, unles you like <em>nano</em>.</li>
<li>and <code>crontab -l</code> to list (piping to <em>less</em>, for example).</li>
</ul>
</li>
</ul>


<p>Note that the command <strong>checks the syntax of the crontab</strong>.</p>

<p>Where to put the cronjobs is up to you. In my case we were using the system crontab but we switched to root&rsquo;s crontab, as we were introducing the cronjobs manually and would like to have a syntax check, just in case.</p>

<h1>Wrapping for manage.py</h1>

<p>To make <strong>manage.py</strong> run you need to provide some <strong>environment variables</strong>. Some of them to activate the virtual environment of your python webapp, others maybe to provide configuration parameters for your webapp (database server, port, credentials, etc.). Also, the python interpreter you should use is the one in your virtualenv. Maybe a wrapper like this in <code>/root/tools/django_cronjobs_wrapper.sh</code> becomes handy:</p>

<pre><code>#!/bin/bash
# Wrapper to run cronjobs

if [ $# -eq 0 ]; then
    echo "usage: $0 {command for manage.py}"
    exit 1
fi

export SETTINGS_FILE='/etc/acme/webapp_settings.conf'
export PYTHONPATH='/opt/acme/webapp_env/'
export PYTHONHOME='/opt/acme/webapp_env'
$PYTHONHOME/bin/python /opt/acme/webapp/code/manage.py $*
</code></pre>

<p>The return value is the one of the manage.py command (useful for cron to know if it failed). <em>Stderr</em> and <em>stdout</em> are not redirected anywhere at the moment.</p>

<p>Indeed, you can extend this wrapper to run <strong>manage.py commands</strong> in the shell, for example a <strong>shell_plus session</strong>, or run a <strong>celery consumer</strong> from <em>Upstart</em> or <em>Supervisor</em>.</p>

<p>Note: I had to change the group of the file to <em>crontab</em> and give it exec. permission:</p>

<pre><code>-rwxr-xr-- 1 root crontab 1055 Aug 7 18:00 tools/django_cronjobs_wrapper.sh
</code></pre>

<h1>Installing the cronjob</h1>

<p>For example:</p>

<pre><code>5 * * * * /root/tools/django_cronjobs_wrapper.sh a_csv_export --for --example &gt;&gt; /var/log/acme/cronjobs/a_csv_export.log
</code></pre>

<p>This would be for a user&rsquo;s crontab. Note that <strong>stdout</strong> goes to a logfile, appending lines, so you will have logs for all the executions (but be careful with the log size!)</p>

<p><strong>stderr</strong> is captured by cron, who will send an email if there were messages.</p>

<p>In case the django command (and all the stuff behind) is properly programmed you will get only emails when there are errors during the cronjob execution (e.g. lack of disk space while writing the CSV file, which will trigger an IOError exception).</p>

<h1>Having a collection of cronjobs</h1>

<p>You may want to keep the crontabs in a repo. Or in a <strong>chef recipe</strong>. Or deploy them in a package (if you deploy the rest of your software with packages).</p>

<p>Some other peopler prefer to use Celery Periodic Tasks, which means that the periodic tasks are configured in the software, instead of using the system configuration.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hypnollama: heading to 1.0.0]]></title>
    <link href="http://pfigue.github.com/blog/2013/08/13/hypnollama-heading-to-1-dot-0-0/"/>
    <updated>2013-08-13T21:03:00+02:00</updated>
    <id>http://pfigue.github.com/blog/2013/08/13/hypnollama-heading-to-1-dot-0-0</id>
    <content type="html"><![CDATA[<p>Well, in the last week finally I could spend some hours working on Hypnollama.</p>

<p>Since around 10 days I wanted to finish some tasks to be able to say that Hypnollama can do something useful (for me).</p>

<p>The last changes are:</p>

<ul>
<li> Moved from <em>httplib</em> to <em>urllib2</em>. Avoids a bug which was preventing me to test HTTPS addresses. I had to refactor the tests and I introduced a class hierarchy.</li>
<li> <a href="http://pfigue.github.io/blog/2013/08/10/hypnollama-new-test-to-check-for-content/">CheckInContent(url, expected_content)</a> to be sure an URL retrieves an exact text.</li>
<li> <a href="http://pfigue.github.io/blog/2013/08/10/hypnollama-checking-for-401-unauthorized/">Unauthorized() test</a> for <strong>401 Unauthorized</strong> restriction in URLs</li>
<li> <strong>Forbidden(url) test</strong>. If Unauthorized() checks for 401, this will check for <strong>403 Forbidden</strong>. I get this error when my IP is blocked in the webserver for an URL.</li>
<li> <strong>CheckREInContent(url, expected_content) test</strong>. Fetches the content of an URL and makes sure a Regular Expression applies to it.</li>
<li> Uses <a href="http://docopt.org/">docopt</a> for parsing the command line arguments. I really like it and recommend to take a look into it.</li>
<li> Output similar to Nosetests. Time spent in testing, number of tests, etc. Only when the tests pass. When they fail, or when errors happen, the output is <em>my own</em>.</li>
<li> I added some extra functional test, but a few more are needed.</li>
</ul>


<p>I would like to prepare a <strong>1.0.0 release</strong> soon.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hypnollama: Checking for 401 Unauthorized]]></title>
    <link href="http://pfigue.github.com/blog/2013/08/10/hypnollama-checking-for-401-unauthorized/"/>
    <updated>2013-08-10T23:59:00+02:00</updated>
    <id>http://pfigue.github.com/blog/2013/08/10/hypnollama-checking-for-401-unauthorized</id>
    <content type="html"><![CDATA[<p>Some of the services that a webserver provides may be restricted to some specific people.</p>

<p>There are two immediate ways to do this from the webserver level:</p>

<ul>
<li> Use <em>Auth Digest</em> which is not very safe, unless it is provided under an SSL channel.</li>
<li> Restrict by IP address.</li>
</ul>


<p>If the webserver configuration opted for <strong>Auth Digest</strong>, we will get an <strong>401 Unauthorized</strong> answer whenever we try to access the ressource without the adequate credentials.</p>

<p>For this reason, I needed a test to be sure that some services are kept only for a few chosen eyes.</p>

<pre><code>Unauthorized(url='http://..../')
</code></pre>

<p>is the test which does the trick.</p>

<p>If it gets a <strong>401 HTTP response</strong>, everything if fine. Test passes.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hypnollama: New test to check for content]]></title>
    <link href="http://pfigue.github.com/blog/2013/08/10/hypnollama-new-test-to-check-for-content/"/>
    <updated>2013-08-10T23:58:00+02:00</updated>
    <id>http://pfigue.github.com/blog/2013/08/10/hypnollama-new-test-to-check-for-content</id>
    <content type="html"><![CDATA[<p>Today I have programmed a new check for Hypnollama.</p>

<p>This one <strong>checks if the retrieved content after accessing an URL matches exactly what we are expecting</strong>.</p>

<p>I had one similar last week, and the difference is that this new one <strong>follows</strong> redirects, while the previous didn&rsquo;t.</p>

<p>Probably I can have used with <em>httplib</em> as I was doing in the past, but it was easier to use <em>urllib2</em> and actually I discovered how I can prevent <em>urllib2</em> to follow redirects when I want (i.e. in most of the tests). So I am moving all tests to <em>urllib2</em></p>

<p>The name for the test is <strong>CheckContent</strong> and can be used like this:</p>

<pre><code>CheckContent(
        url='http://www.twitter.com/robots.txt',
        expected_content=\
'''User-agent: *
Disallow: /
''')
</code></pre>

<p>I think it doesn&rsquo;t need explanations.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting talks at conferences]]></title>
    <link href="http://pfigue.github.com/blog/2013/03/28/interesting-talks-at-conferences/"/>
    <updated>2013-03-28T16:02:00+01:00</updated>
    <id>http://pfigue.github.com/blog/2013/03/28/interesting-talks-at-conferences</id>
    <content type="html"><![CDATA[<ul>
<li><a href="http://www.shmoocon.org/2013/videos/Shmoocon%202013%20-%20C10M%20Defending%20The%20Internet%20At%20Scale.mp4">C10M Defending The Internet At Scale &ndash; Robert Graham &ndash; Shmoocon 2013</a></li>
<li><a href="http://www.shmoocon.org/2013/videos/Shmoocon%202013%20-%20How%20Smart%20Is%20BlueTooth%20Smart.mp4">How Smart is the Bluetooth Smart &ndash; Mike Ryan &ndash; Shmoocon 2013</a> (<a href="http://lacklustre.net/bluetooth/">more info</a>)</li>
<li><a href="http://conference.hitb.org/hitbsecconf2013ams/materials/D1T2%20-%20Daniel%20Mende%20-%20Paparazzi%20Over%20IP.pdf">Paparazzi Over IP &ndash; Daniel Mendez &ndash; HitB SecConf 2013 Amsterdan</a></li>
<li><a href="http://www.thotcon.org/archive/0x1presos/09_THOTCON_0x1-Virus_Writing_Techniques-Sally.pdf">Virus Writing Techniques &ndash; Sally &ndash; Thotcon 0x1</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Read-the-Docs: served standalone with uwsgi]]></title>
    <link href="http://pfigue.github.com/blog/2013/03/23/read-the-docs-served-standalone-with-uwsgi/"/>
    <updated>2013-03-23T21:08:00+01:00</updated>
    <id>http://pfigue.github.com/blog/2013/03/23/read-the-docs-served-standalone-with-uwsgi</id>
    <content type="html"><![CDATA[<p>Once you did the <a href="http://pfigue.github.com/blog/2013/03/23/installing-read-the-docs-for-development/">development installation for Read The Docs</a>, you should <a href="https://docs.djangoproject.com/en/1.4/howto/deployment/wsgi/">prepare a wsgi.py file for the django webapp</a>, like this:</p>

<pre><code>import os

os.environ.setdefault("DJANGO_SETTINGS_MODULE", "readthedocs.settings")

# This application object is used by the development server
# as well as any WSGI server configured to use this file.
from django.core.wsgi import get_wsgi_application
application = get_wsgi_application()
</code></pre>

<p>The <em>redis-cache</em> is a requirement when running with wsgi, with the virtualenv activated:</p>

<pre><code># pip install django-redis-cache
</code></pre>

<p>In case you opt for <em>uwsgi</em> instead of <em>gunicorn</em>, install it in the virtualenv:</p>

<pre><code># pip install uwsgi
</code></pre>

<p>Now you should be ready to launch uwsgi for development:</p>

<pre><code># DJANGO_SETTINGS_MODULE='readthedocs.settings.sqlite' /opt/readthedocs.org/.venv/bin/uwsgi --home /opt/readthedocs.org/.venv/ --module readthedocs.wsgi --socket 127.0.0.1:8001 --pidfile /var/run/uwsgi-rtfm.localhost.pid --processes 1 --master --max-requests 50 --pythonpath /opt/readthedocs.org/ --reload-on-rss 256 --reload-on-as 256 --limit-as 384 --harakiri 360
</code></pre>

<p>You shouldn&rsquo;t be able to make an HTTP request as it is using the WSGI protocol.</p>

<p>Once that is working, you can configure <em>nginx</em> to forward the connections to the <em>uwsgi</em>.</p>

<p>First, install nginx:</p>

<pre><code># apt-get install nginx-full
</code></pre>

<p>Follow configuring a new <em>virtual host</em> in the configuration:</p>

<pre><code>cat &gt; /etc/nginx/site-enabled/read-the-docs.localhost &lt; EOF
server {
    listen 80;
    server_name read-the-docs.localhost;
    access_log  /opt/readthedocs.org/logs/read-the-docs.localhost.log;

    location / {
        uwsgi_pass 127.0.0.1:8001;
        include uwsgi_params;
    }
}
EOF
</code></pre>

<p><em>Note</em> that <em>uwsgi</em> is listening in the the port 8001, where <em>nginx</em> is forwarding the requests using the <em>WSGI</em> protocol.</p>

<p>and path the <em>/etc/hosts</em> adding this alias:</p>

<pre><code>127.0.0.1   read-the-docs.localhost
</code></pre>

<p>Test and reload the nginx config:</p>

<pre><code># /etc/init.d/nginx testconfig
# /etc/init.d/nginx reload
</code></pre>

<p>And now with a browser just <a href="http://read-the-docs.localhost/">visit it</a> and check if you get the expected result.</p>

<h2>References</h2>

<ul>
<li><a href="https://docs.djangoproject.com/en/1.4/howto/deployment/wsgi/">Django: How to deploy with WSGI</a></li>
<li><a href="https://docs.djangoproject.com/en/dev/howto/deployment/wsgi/uwsgi/">Django: How to use Django with uWSGI</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
